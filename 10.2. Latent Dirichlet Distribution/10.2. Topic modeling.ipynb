{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paragraphs: 2190\n",
      "\n",
      "First paragraph:\n",
      " ï»¿The Project Gutenberg EBook of Pride and Prejudice, by Jane Austen\n",
      "\n",
      "Last paragraph:\n",
      " This Web site includes information about Project Gutenberg-tm, including how to make donations to the Project Gutenberg Literary Archive Foundation, how to help produce our new eBooks, and how to subscribe to our email newsletter to hear about new eBooks.\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "with open('Pride and Prejudice.txt', encoding = 'utf-8') as myfile:\n",
    "    collection = myfile.read().split(\"\\n\\n\") #\\n\\n denotes the double linebreak used for paragraph separation\n",
    "    \n",
    "for i in range(len(collection)):\n",
    "    #replace \"\\n\" in list items with a space\n",
    "    collection[i] = collection[i].replace('\\n',' ')\n",
    "    #collection[i] = collection[i].replace(\"\\'\",'')\n",
    "    \n",
    "collection = [i.strip() for i in collection if len(i) > 1]\n",
    "\n",
    "print(f\"Number of paragraphs: {len(collection)}\\n\")\n",
    "print(f\"First paragraph:\\n {collection[0]}\\n\")\n",
    "print(f\"Last paragraph:\\n {collection[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('Pride and Prejudice.txt', encoding = 'utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2394\n",
      "1960\n"
     ]
    }
   ],
   "source": [
    "def clean_word(text): \n",
    "    for word in stopwords.words('english'): \n",
    "        text = text.replace(' '+word+' ',' ')\n",
    "    puncs = ['-', ':', '.', ';', ',', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '(', ')', '?', '[', ']', '!', '/', \"'\"]\n",
    "    for _ in puncs:\n",
    "        text = text.replace(_,'')\n",
    "    return text\n",
    "\n",
    "def make_paragraphs(text):\n",
    "    paragraphs = []\n",
    "    dumb = ''\n",
    "    for _ in range(len(text)): \n",
    "        if text[_] == '\\n':\n",
    "            paragraphs.append(dumb)\n",
    "            dumb = ''\n",
    "            continue\n",
    "        else: \n",
    "            dumb += text[_]\n",
    "            \n",
    "    for i in range(len(paragraphs)):\n",
    "        paragraphs[i] = paragraphs[i].replace('\\n', ' ').lower()\n",
    "        paragraphs[i] = clean_word(paragraphs[i])\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "paragraphs = make_paragraphs(corpus)\n",
    "print(len(paragraphs))\n",
    "new_paragraphs = []\n",
    "for _ in range(len(paragraphs)): \n",
    "    if len(paragraphs[_]) > 30:\n",
    "        new_paragraphs.append(paragraphs[_])\n",
    "print(len(new_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=100,\n",
    "                                stop_words='english')\n",
    "vectors = vectorizer.fit_transform(new_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=15, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=25, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=15, max_iter=10,\n",
    "                                learning_method='batch',\n",
    "                                random_state=25)\n",
    "lda.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: project work received way away place long hope come wish\n",
      "Topic #1: bingley mr sisters jane sister netherfield friend elizabeth till attention\n",
      "Topic #2: cried oh lydia wickham certainly heard father dear left know\n",
      "Topic #3: said hope subject sister feelings replied soon elizabeth away felt\n",
      "Topic #4: lady catherine wickham mr young collins quite manner daughter make\n",
      "Topic #5: miss bennet bingley house thought darcy elizabeth said seen soon\n",
      "Topic #6: make happy love sure quite good dear daughter hear said\n",
      "Topic #7: elizabeth saw said jane room looked morning came evening pleasure\n",
      "Topic #8: say long come opinion jane mother said happiness shall sister\n",
      "Topic #9: think wish like great believe world said really know place\n",
      "Topic #10: darcy mr man elizabeth manner said time young wish colonel\n",
      "Topic #11: know lizzy shall tell dear colonel think added aunt told\n",
      "Topic #12: mr collins sir elizabeth charlotte lucas friend room house mrs\n",
      "Topic #13: letter little family longbourn marriage soon time felt pleasure elizabeth\n",
      "Topic #14: mrs bennet mr gardiner kitty soon said lydia family daughter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "names = vectorizer.get_feature_names()\n",
    "print_top_words(lda, names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33438773 0.82925212 0.86288963 0.90270716 0.96534238 0.99518203\n",
      " 0.99959159 0.99959902 0.99971906]\n",
      "Counter({1.0: 4930, 0.0: 3375, 4.0: 621, 3.0: 382, 2.0: 343, 5.0: 317, 6.0: 29, 7.0: 1, 8.0: 1, 9.0: 1})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "stick_breaks = np.asarray([])\n",
    "\n",
    "\n",
    "def sample_stick_breaking_process(stick_breaks):\n",
    "    s = np.random.rand()\n",
    "    lst = np.where(s < stick_breaks)[0]\n",
    "    if lst.size:\n",
    "        return (stick_breaks, lst[0])\n",
    "    else:\n",
    "        # We have sampled a new category!\n",
    "        # Break the stick in a new place and update the array!\n",
    "        if stick_breaks.size == 0:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = stick_breaks[-1]\n",
    "        width = 1.0 - start\n",
    "        pos = start + width * np.random.rand()\n",
    "        stick_breaks = np.append(stick_breaks, [pos])\n",
    "        return (stick_breaks, len(stick_breaks))\n",
    "\n",
    "\n",
    "N = 10000\n",
    "samples = np.zeros(N)\n",
    "for n in range(N):\n",
    "    (stick_breaks, samples[n]) = sample_stick_breaking_process(stick_breaks)\n",
    "\n",
    "print(stick_breaks)\n",
    "print(Counter(samples))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
